{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "import tensorflow.keras.backend as kb\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pysam\n",
    "from scipy.special import logsumexp\n",
    "import imageio\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class CustomModel(Model):\n",
    "\n",
    "    def __init__(self, num_tasks, tracks_for_each_task, output_profile_len, loss_weights,counts_loss, **kwargs):\n",
    "\n",
    "        # call the base class with inputs and outputs\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # number of tasks\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # number of tracks for each task\n",
    "        self.tracks_for_each_task = tracks_for_each_task\n",
    "        \n",
    "        # output profile length\n",
    "        self.output_profile_len = output_profile_len\n",
    "        \n",
    "        # weights for the profile mnll and logcounts losses\n",
    "        self.loss_weights = loss_weights\n",
    "        \n",
    "        # logcounts loss funtion\n",
    "        self.counts_loss = counts_loss\n",
    "        \n",
    "        # object to track overall mean loss per epoch\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "    def _get_loss(self, x, y, sample_weights, training=True):\n",
    "        # boolean mask for sample weights != 0\n",
    "        \n",
    "                \n",
    "        y_pred = self(x, training=training)  # Forward pass\n",
    "        \n",
    "        \n",
    "        def poisson_loss_function(y_log_true, y_log_pred):\n",
    "            # we can use the Possion PMF from TensorFlow as well\n",
    "            # dist = tf.contrib.distributions\n",
    "            # return -tf.reduce_mean(dist.Poisson(y_pred).log_pmf(y_true))\n",
    "\n",
    "            # last term can be avoided since it doesn't depend on y_pred\n",
    "            # however keeping it gives a nice lower bound to zero\n",
    "            \n",
    "            y_true = tf.math.exp(y_log_true)\n",
    "            y_pred = tf.math.exp(y_log_pred)\n",
    "            y_true = tf.cast(y_true,tf.float32)\n",
    "            y_pred = tf.cast(y_pred,tf.float32)\n",
    "            loss = y_pred - y_true*tf.math.log(y_pred+1e-8) + tf.math.lgamma(y_true+1.0)\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        def _poisson_loss_function(_y_log_true,_y_log_pred):\n",
    "            total_poisson_loss = 0\n",
    "            track_count_cuml = 0\n",
    "            num_tasks_count_cuml = 0\n",
    "            for i in range(self.num_tasks):\n",
    "                num_of_tracks = self.tracks_for_each_task[i]\n",
    "                y_log_true = tf.reduce_logsumexp(_y_log_true[:,track_count_cuml:(track_count_cuml+num_of_tracks)],axis=1)\n",
    "                y_log_pred = tf.reduce_logsumexp(_y_log_pred[:,num_tasks_count_cuml:(num_tasks_count_cuml+1)],axis=1)\n",
    "                \n",
    "                loss = poisson_loss_function(y_log_true, y_log_pred)\n",
    "                track_count_cuml += num_of_tracks\n",
    "                num_tasks_count_cuml += 1\n",
    "                total_poisson_loss += loss\n",
    "            return total_poisson_loss\n",
    "    \n",
    "        def mse_loss_function(y_log_true, y_log_pred):\n",
    "            # logcounts mse loss without sample weights\n",
    "            mse_loss = keras.losses.mean_squared_error(\n",
    "                y_log_true, y_log_pred)\n",
    "            return mse_loss\n",
    "        \n",
    "        def _mse_loss_function(_y_log_true,_y_log_pred):\n",
    "            total_mse_loss = 0\n",
    "            track_count_cuml = 0\n",
    "            num_tasks_count_cuml = 0\n",
    "            for i in range(self.num_tasks):\n",
    "                num_of_tracks = self.tracks_for_each_task[i]\n",
    "                y_log_true = tf.reduce_logsumexp(_y_log_true[:,track_count_cuml:(track_count_cuml+num_of_tracks)],axis=1)\n",
    "                y_log_pred = tf.reduce_logsumexp(_y_log_pred[:,num_tasks_count_cuml:(num_tasks_count_cuml+1)],axis=1)\n",
    "                \n",
    "                loss = mse_loss_function(y_log_true, y_log_pred)\n",
    "                track_count_cuml += num_of_tracks\n",
    "                num_tasks_count_cuml += 1\n",
    "                total_mse_loss += loss\n",
    "            return total_mse_loss\n",
    "        \n",
    "        if self.counts_loss == \"MSE\":\n",
    "            total_counts_loss = _mse_loss_function(y['logcounts_predictions'],y_pred[1])\n",
    "        \n",
    "        elif self.counts_loss == \"POISSON\":\n",
    "        \n",
    "            total_counts_loss = _poisson_loss_function(y['logcounts_predictions'],y_pred[1])\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Sorry, unknown loss funtion\")\n",
    "        \n",
    "        boolean_mask = tf.math.greater_equal(sample_weights, 1.0)\n",
    "\n",
    "        # for mnll loss we mask out samples with weight == 0.0\n",
    "        \n",
    "        _y = tf.boolean_mask(y['profile_predictions'], boolean_mask)\n",
    "        _y_pred = tf.boolean_mask(y_pred[0], boolean_mask)\n",
    "\n",
    "        def _zero_constant():\n",
    "            return kb.constant(0)\n",
    "        def multinomial_nll(true_counts, logits):\n",
    "            \"\"\"Compute the multinomial negative log-likelihood\n",
    "            Args:\n",
    "              true_counts: observed count values\n",
    "              logits: predicted logits values\n",
    "            \"\"\"\n",
    "            counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "            dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                                 logits=logits)\n",
    "            return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "                    tf.cast(tf.shape(true_counts)[0], dtype=tf.float32))\n",
    "    \n",
    "        def _multinomial_nll(_y,_y_pred):\n",
    "            total_mnll_loss = 0\n",
    "            track_count_cuml = 0\n",
    "            for i in range(self.num_tasks):\n",
    "                num_of_tracks = self.tracks_for_each_task[i]\n",
    "                _y_reshape = tf.reshape(\\\n",
    "                                        _y[:,:,track_count_cuml:(track_count_cuml+num_of_tracks)],\\\n",
    "                                        [-1,(num_of_tracks)*(self.output_profile_len)]\\\n",
    "                                       )\n",
    "                _y_pred_reshape = tf.reshape(\\\n",
    "                                             _y_pred[:,:,track_count_cuml:(track_count_cuml+num_of_tracks)],\\\n",
    "                                             [-1,(num_of_tracks)*(self.output_profile_len)]\\\n",
    "                                            )\n",
    "                \n",
    "                loss = multinomial_nll(_y_reshape, _y_pred_reshape)\n",
    "                track_count_cuml = track_count_cuml+num_of_tracks\n",
    "                total_mnll_loss += loss\n",
    "            return total_mnll_loss\n",
    "                    \n",
    "        total_mnll_loss = tf.cond(tf.equal(tf.size(_y), 0), \n",
    "                  _zero_constant,\n",
    "                  lambda:  _multinomial_nll(_y,_y_pred))\n",
    "        \n",
    "        if self.counts_loss == \"MSE\":\n",
    "            loss =  (self.loss_weights[0] * total_mnll_loss) + \\\n",
    "                (self.loss_weights[1] * total_counts_loss)        \n",
    "        elif self.counts_loss == \"POISSON\":\n",
    "        \n",
    "            loss =  total_mnll_loss + total_counts_loss            \n",
    "        else:\n",
    "            raise Exception(\"Sorry, unknown loss funtion\")\n",
    "\n",
    "        return loss, total_mnll_loss, total_counts_loss\n",
    "            \n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weights = data\n",
    "        print(kb.int_shape(x['sequence']))\n",
    "        print(kb.int_shape(y['profile_predictions']))\n",
    "        print(kb.int_shape(y['logcounts_predictions']))\n",
    "        print(kb.int_shape(sample_weights))        \n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, total_mnll_loss, total_counts_loss = \\\n",
    "                self._get_loss(x, y, sample_weights)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result(),\n",
    "                \"batch_loss\": loss,\n",
    "                \"profile_predictions_loss\": total_mnll_loss, \n",
    "                \"logcounts_predictions_loss\": total_counts_loss}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "    \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y, sample_weights = data\n",
    "        \n",
    "        loss, total_mnll_loss, total_counts_loss = \\\n",
    "            self._get_loss(x, y, sample_weights, training=False)\n",
    "            \n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result(),\n",
    "                \"batch_loss\": loss,\n",
    "                \"profile_predictions_loss\": total_mnll_loss, \n",
    "                \"logcounts_predictions_loss\": total_counts_loss}\n",
    "    \n",
    "def get_model(model_path):\n",
    "    with CustomObjectScope({'MultichannelMultinomialNLL': lambda n='0':n,\n",
    "                            \"kb\": kb,\n",
    "                            \"CustomMeanSquaredError\":lambda n='0':n,\n",
    "                            \"tf\":tf,\n",
    "                           \"CustomModel\":CustomModel}):\n",
    "        model = load_model(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def random_seq(seqlen):\n",
    "    return ''.join(random.choices(\"ACGT\", k=seqlen))\n",
    "\n",
    "\n",
    "\n",
    "def fix_sequence_length(sequence, length):\n",
    "    \"\"\"\n",
    "        Function to check if length of sequence matches specified\n",
    "        length and then return a sequence that's either padded or\n",
    "        truncated to match the given length\n",
    "        Args:\n",
    "            sequence (str): the input sequence\n",
    "            length (int): expected length\n",
    "        Returns:\n",
    "            str: string of length 'length'\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the sequence is smaller than expected length\n",
    "    if len(sequence) < length:\n",
    "        # pad the sequence with 'N's\n",
    "        sequence += 'N' * (length - len(sequence))\n",
    "    # check if the sequence is larger than expected length\n",
    "    elif len(sequence) > length:\n",
    "        # truncate to expected length\n",
    "        sequence = sequence[:length]\n",
    "\n",
    "    return sequence\n",
    "def one_hot_encode(sequences, seq_length):\n",
    "    \"\"\"\n",
    "    \n",
    "       One hot encoding of a list of DNA sequences \n",
    "       \n",
    "       Args:\n",
    "           sequences (list): python list of strings of equal length\n",
    "           seq_length (int): expected length of each sequence in the \n",
    "               list\n",
    "           \n",
    "       Returns:\n",
    "           numpy.ndarray: \n",
    "               3-dimension numpy array with shape \n",
    "               (len(sequences), len(list_item), 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        logging.error(\"'sequences' is empty\")\n",
    "        return None\n",
    "    \n",
    "    # First, let's make sure all sequences are of equal length\n",
    "    sequences = list(map(\n",
    "        fix_sequence_length, sequences, [seq_length] * len(sequences)))\n",
    "\n",
    "    # Step 1. convert sequence list into a single string\n",
    "    _sequences = ''.join(sequences)\n",
    "    \n",
    "    # Step 2. translate the alphabet to a string of digits\n",
    "    transtab = str.maketrans('ACGTNYRMSWK', '01234444444')    \n",
    "    sequences_trans = _sequences.translate(transtab)\n",
    "    \n",
    "    # Step 3. convert to list of ints\n",
    "    int_sequences = list(map(int, sequences_trans))\n",
    "    \n",
    "    # Step 4. one hot encode using int_sequences to index \n",
    "    # into an 'encoder' array\n",
    "    encoder = np.vstack([np.eye(4), np.zeros(4)])\n",
    "    X = encoder[int_sequences]\n",
    "\n",
    "    # Step 5. reshape \n",
    "    return X.reshape(len(sequences), len(sequences[0]), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = get_model(model_path='/oak/stanford/groups/akundaje/vir/tfatlas/models/round3/ENCSR302AWT/ENCSR302AWT_split000.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "output_seq_len = 1000\n",
    "number_of_strands = 2\n",
    "\n",
    "def random_seq(seqlen):\n",
    "    return ''.join(random.choices(\"ACGT\", k=seqlen))\n",
    "\n",
    "encoded_inserted_sequences = one_hot_encode([random_seq(2114) for i in range(900)],2114)\n",
    "\n",
    "predictions = model.predict([encoded_inserted_sequences,\n",
    "               np.zeros(output_seq_len*number_of_strands*encoded_inserted_sequences.shape[0]).reshape((encoded_inserted_sequences.shape[0],output_seq_len,number_of_strands)),    \n",
    "               np.zeros(encoded_inserted_sequences.shape[0]*number_of_strands).reshape((encoded_inserted_sequences.shape[0],number_of_strands))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "batch_size = 900\n",
    "output_len = 1000\n",
    "num_output_tracks = 2\n",
    "\n",
    "#option 1\n",
    "pred_profiles = np.zeros((batch_size, output_seq_len, num_output_tracks))\n",
    "pred_logcounts = np.zeros((batch_size, num_output_tracks))\n",
    "\n",
    "for idx in range(batch_size):\n",
    "        for j in range(num_output_tracks):\n",
    "        # combined counts prediction from the count head\n",
    "            logcounts_prediction = predictions[1][idx] # this is the logsum of both the strands that is \n",
    "                                                       # predicted                \n",
    "\n",
    "            # predicted profile — for now assuming that there are two strands and only one task\n",
    "            pred_profile_logits = np.reshape(predictions[0][idx, :, :],[1,output_len*2])\n",
    "\n",
    "            profile_predictions = (np.exp(\\\n",
    "                                          pred_profile_logits - \\\n",
    "                                          logsumexp(\\\n",
    "                                                    pred_profile_logits\\\n",
    "                                                   )) * (np.exp(logcounts_prediction))\\\n",
    "                                  )\n",
    "\n",
    "            pred_profiles[idx, :, j] = np.reshape(profile_predictions,[output_len,2])[:,j]\n",
    "\n",
    "            # counts prediction\n",
    "            pred_logcounts[idx, j] = np.log(np.sum(np.reshape(profile_predictions,\\\n",
    "                                                                [output_len,2]\\\n",
    "                                                              )[:,j]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2\n",
    "def vectorized_prediction_to_profile(predictions):\n",
    "        logits_arr = predictions[0]\n",
    "        counts_arr = predictions[1]\n",
    "        pred_profile_logits = np.reshape(logits_arr,[-1,1,output_len*2])\n",
    "        probVals_array = np.exp(pred_profile_logits-logsumexp(pred_profile_logits,axis=2).reshape([len(logits_arr),1,1]))\n",
    "        profile_predictions = np.multiply((np.exp(counts_arr)).reshape([len(counts_arr),1,1]),probVals_array)\n",
    "        plus = np.reshape(profile_predictions,[len(counts_arr),output_len,2])[:,:,0]\n",
    "        minus = np.reshape(profile_predictions,[len(counts_arr),output_len,2])[:,:,1]\n",
    "        return(plus,minus)\n",
    "    \n",
    "plus,minus = vectorized_prediction_to_profile(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_profiles[:,:,0]==vectorized_prediction_to_profile(predictions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_profiles[:,:,1]==vectorized_prediction_to_profile(predictions)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_profiles[:,:,0]==vectorized_prediction_to_profile(predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred_profiles[:,:,1]==vectorized_prediction_to_profile(predictions)[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
